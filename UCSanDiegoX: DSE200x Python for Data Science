1)UNIVERSITY COURSE CREDIT:
   edX's MicroMasters Credential program allows MOOC learners, like you, to get credit towards courses for a Masters degree on various university campuses (affiliated with edX).  
   Current Masters Options:
  Currently, learners who successfully earn the MicroMasters Credential are eligible to apply for admission to the Master of Predictive Analytics program at Curtin University.
  If a learner applies for admission to the Master of Predictive Analytics program at Curtin University, and is accepted, the MicroMasters Credential will count towards 25% (or 100 credits of the 400 credits) of the coursework required for graduation in the Curtin program.
  Future Masters Options:
  At UCSD, we are committed to giving learners everywhere access to the quality, state-of-the-art data science training and preparation we provide our own Masters students in San Diego.  We plan to work with other edX partner universities with appropriate Masters programs to encourage them to accept our MicroMasters courses for credit within their own programs.  We'll update this page as we do this. These courses are designed directly from our own courses on campus and we believe they serve as an excellent foundation for many Masters programs preparing data scientists.

2)For those of you interested in a career in data science, this is the first course in a four-course edX MicroMasters.
The next three courses are Statistics, Machine Learning, and Spark (data analysis for big data). 

3)To receive a MicroMasters Certificate in Data Science you must receive verified certificates in the following courses:
    Python for Data Science
   Statistics and Probability in Data Science using Python
   Machine Learning for Data Science
   Big Data Analytics using Spark
  Learners who receive their MicroMasters in Data Science Certificate are currently eligible to apply to the Master of Predictive Analytics program at Curtin University.A MicroMasters in Data Science can be converted into a maximum of 100 units in this program as directed by the Curtin program course listing below.
    MEDA5003 Multidisciplinary Data Visualization and Interpretation (25 credits)
    STAT5001 Statistical Probability (12.5 credits)
    STAT5006 Statistical Data Analysis 1 (12.5 credits)
    ISYS5007 Data Management (25 credits)
    Elective (25 credits)
-----------------------------------------------------------------------------
1)why data scients like python for DS:
   the Jupyter Notebooks make Python-based analysis more producible and repeatable
   NumPy and Pandas to ingest and analyze data efficiently.
   We will add the visualization libraries including Matplotlib,
   scikit_learn to apply ML 
   beautifulsoup to read XML and HTML type data
2)There are five key steps in the overall process of data science,namely, (these 5 steps ar kind of an iterative process)
     a) data acquisition, (importing raw dataset into our analytics platform) 
      b)data preparation, (explore and visualize , perform data cleaning)
      c)data analysis, (use statistical analysis and ML i.e feature selection,model selection and finding the result)
      d)presentation, and (present your findings)
      e)reporting of insights and turning these insights into data-driven actions.(purpose i.e stake holders will take actions and use these findings)
**Though we have five steps, first step is always the purpose/reason of collecting the data i.e to solve which problem we are doing this
a)Data Acquisition:
   -->we may need to ingest/take data from DBs(relational and non-relational(NoSqls)),
      text files(csv files,test files),live feeds(sensors,online platforms(twitter,live feed of weather observations)
      
b)Data preparation/exploration:
    -->Correlation graphs: can be used to explore the dependencies between different variables in the data.
    -->General trends: show you a simple graph of how the data is progressing over time.
    -->Outliers show you the data points that are distant from other data points.Plotting outliers will help you double-check for errors in the data due to measurement.
       In some cases, outliers that are not errors might make you find a rare event.
    -->summary statistics(like mean,median,mode,range,standard deviation etc) :provide the numerical values to describe your data.Summary statistics are quantities that capture various characteristics of a set of values.
       Mean and median are measures of the location of specific values.
       Mode is the value that occurs most frequently in your data set.
       range and standard deviation are measures of spread in your data.
   Looking at these measures will give you an idea of the nature of your data.They can tell you if there's something wrong with your data.
   For example, if the range of the values for age in your data includes negative numbers or a number much greater than hundred, there's something suspicious in the data
   -->Visualization techniques: also provide quick and effective,overall, a very useful way to look at data in this preliminary analysis step.   
      A heat map, for instance, an quickly give you an idea where the hot spots are.
      Many different types of graphs can be used.
         Histograms show the distribution of the data and can show skewness or unusual dispersion.
         Boxplots are another type of plot for showing data distribution.
         Line graphs are useful for seeing how values in your data change over time.Spikes in the data are also easy to spot.
         Scatter plots can show correlation between two variables.
    -->get clean/quality data
         a)remove data with missing values
         b)merge duplicate records(This would record a way to determine how to resolve conflicting values.Perhaps it makes sense to retain the nearer value whenever there's a conflict.)
         c)generate best estimate for an invalid value(eg : we can assume age of an employee when there is no/invalid age given based on the time he joined the company and his role etc)
         d)remove outliers
    ---> get data into shape(also called as data manipulation or data pre-processing or data wrangling or data munging.)
         a)This is the 2nd part of data preparation
         b)Some operations for this data munging,wrangling, pre-processing includes scaling, transformation,feature selection, dimensionality reduction and data manipulation.
         c)Scaling involves changing range of values to be between a specified range such as from zero to one.This is done to avoid having certain features with large values from dominating the results.
         d)Feature selection can involve removing redundant or irrelevant features,combining features and creating new features.
         e)Various transformations can be performed on the data to reduce noise and variability.One such transformation is called aggregation.Aggregate data generally results in data with less variability,which may help with the analysis in the long term.
           Aggregating values to weekly or monthly sales figures will result in smoother data.Of course, this comes at the cost of less detailed data,so these factors must be weighed for the specific application.
         f)A technique commonly used for dimensionality reduction is called principal component analysis(to reduce the no. of dimentions,when dataset has more dimensions)
c)Data Analysis:
    -->Data analysis involves building a model from your data(input data).
    -->The main categories of analysis techniques are classification, regression, clustering,association analysis and graph analysis.
         a)Classification:In classification,the goal is to predict the category of the input data.(Eg:predicting the weather as being sunny, rainy, windy or cloudy.if we have to classify the data into only 2 categories ==> it is called binary classification)
         b)Regression:When your model has to predict a numeric value instead of a category,then the task becomes a regression problem.
         c)Clustering:The goal is to organize similar items into groups.An example is grouping a company's customer base into distinct segments for more effective targeted marketing like seniors, adults and teenagers as we see here.
         d)Association Analysis:The goal in association analysis is to come up with a set of rules to capture associations between items or events.The rules are used to determine when items or events occur together.A common application of association analysis is known as market basket analysis.which is used to understand customer purchasing behavior.For example,association analysis can reveal that banking customers who have certificate of deposit accounts, shortly CDs,also tend to be interested in other investment vehicles
         e)Graph Analytics:This kind of data comes about when you have a lot of entities and connections between those entities like social networks.Some examples where graph analytics can be useful are exploring the spread of a disease or epidemic by analyzing hospitals and doctors' records, identification of security threats by monitoring social media, email and text data,and optimization of mobile communication network traffic to ensure data quality and reduce dropped calls.  
         f)Modelling:Modeling starts with selecting one of these techniques we listed as the appropriate analysis technique depending on the type of problem you have.Then,you construct the model using the data that you've prepared.To validate the model,you apply it to new data samples.This is to evaluate how well the model does on data that was used to construct it.
    -->Evaluating the model depends on the type of analysis technique you used.Let's briefly look at how to evaluate each technique.
         a)For classification and regression you will have the correct output for each sample in your input data.Comparing the correct output and the output predicted by the model provides a way to evaluate the model.
         b)For clustering,the groups resulting from clustering should be examined to see if they make sense for your application.
         c)For association analysis and graph analysis some investigation will be needed to see if the results are correct.For example,network traffic delays need to be investigated to see if what your model predicts is actually happening,and whether the sources of the delays are where they are predicted to be in the real system.
    -->After you have evaluated your model to get a sense of its performance on your data,you will be able to determine the next steps.Some questions to consider are:
         a)should the analysis be performed with more data in order to get better model performance? 
         b)Would using different data help? For example, in your clustering results,is it difficult to distinguish customers from distinct regions? Or would adding zip codes to your input data to help generate final grain customer segments is needed?
         c)Do the analysis results suggest a more detailed look at some aspect of the problem?For example, predicting sunny weather gives very good results but rainy weather predictions are just so so.This means you should take a closer look at your examples for rainy weather.Perhaps there are some anomalies in those samples or perhaps there are some missing data that needs to be included in order to completely capture rainy weather.
       The ideal situation would be that your model performs very well with respect to success criteria that were determined when you defined a problem at the beginning of the project.

d)Reporting Insites/Presenting:
   Techs generally used for visualizations:
                  1)R,
                  2)Python,
                  3)D3 is a JavaScript library for producing interactive web-based visualizations and data-driven comments,
                  4)Leaflet is a lightweight, mobile-friendly JavaScript library to create interactive maps.
                  5)Lastly, Tableau and Google Charts allow you to create visualizations in your profiles so you can share them or put them on a site or a blog, and they provide cross-platform compatibility to mobile devices.
                  6)Timeline is a JavaScript library that allows you to create timelines over these results.


Questions:
1)what is websocket service?
Sol: similar to REST but it transfers the data in real time (i.e as soon as an event occurs it transfers data.. eg : transfering data from weather towers)
